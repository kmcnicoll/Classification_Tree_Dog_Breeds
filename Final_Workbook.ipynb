{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector \n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import config\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "%matplotlib inline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "from pydotplus import graph_from_dot_data\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnx = mysql.connector .connect(\n",
    "    host = config.host,\n",
    "    user = config.user,\n",
    "    passwd = config.passwd,\n",
    "    database= 'Dog_Breeds'\n",
    ")\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# cursor.close()\n",
    "# cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq= \"\"\"SELECT * FROM Dogs\"\"\"\n",
    "cursor.execute(sq)\n",
    "sql=cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naming Columns\n",
    "df= pd.DataFrame(sql)\n",
    "df.columns = [\"Name\", \"Male_Min_Weight\", \"Female_Min_Weight\", \"Male_Max_Weight\", \"Female_Max_Weight\",\"Male_Height\", \"Female_Height\", \"Exercise_Min_Min_Daily\", \n",
    "              \"Exercise_Max_Min_Daily\",\n",
    "    \"Energy_Level\", \"Longevity_Min\", \"Longevity_Max\",\n",
    "              \"Tend_Drool\", \"Tend_Snore\", \"Tend_Bark\", \"Tend_Dig\", \"Attention_Needs\", \"Grooming_Needs\", \"AKC_Class\", \"UKC_Class\", \"Prevalence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run code Twice, once with non AKC as a labaled \"Not AKC classified.\"\n",
    "df=df[df.AKC_Class != '<p> <strong>UKC Classification</strong>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "\n",
    "for column in list(df.columns):\n",
    "    plt.figure(figsize = (10,5))\n",
    "    sns.countplot(df[column], alpha =.80)\n",
    "#     plt.title('Survivors vs Non-Survivors')\n",
    "    plt.ylabel('# counts')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing low medium high ranking to 0 ,1 ,2 respectivly\n",
    "\n",
    "df['Tend_Drool'] = df['Tend_Drool'].map({'Low': 0, 'Moderate': 1, \"High\": 2})\n",
    "df['Tend_Snore'] = df['Tend_Snore'].map({'Low': 0, 'Moderate': 1, \"High\": 2})\n",
    "df['Tend_Dig'] = df['Tend_Dig'].map({'Low': 0, 'Moderate': 1, \"High\": 2})\n",
    "df['Attention_Needs'] = df['Attention_Needs'].map({'Low': 0, 'Moderate': 1, \"High\": 2})\n",
    "df['Tend_Bark'] = df['Tend_Bark'].map({'Low': 0, 'Moderate': 1, \"High\": 2, \"Unknown\": 1})\n",
    "df['Grooming_Needs'] = df['Grooming_Needs'].map({'Low': 0, 'Moderate': 1, \"High\": 2, \"Unknown\": 1})\n",
    "\n",
    "# breeds with years were checked by hand. terrier and working respectivly\n",
    "df['Energy_Level'] = df['Energy_Level'].map({'Very energet': 2, 'Average': 1, \"Laid back \": 0, \"Very Energet\": 2, \"Average.\": 1, \"high\": 2, \n",
    "                                             \"Laid Back\": 0, \"Moderate\": 1, \"Active\": 2, \"12-14 yrs\": 1, \"Bred to work\": 2, \"High\": 2, \"low to moder\": 0, \"10-13 yrs.\": 2 })                                                                         \n",
    "                                             \n",
    "df['Prevalence'] = df['Prevalence'].map({'Common': 0, 'So-so': 1, \"Rare\": 2, \"Common</p>\": 0, \"So-so</p>.\": 1, \"Common.\": 0,\n",
    "                                         \n",
    "     \"So-So.\": 1, \n",
    "    \"So-so.\": 1, \"Rare.\": 2, \"So-so.</p>\": 1, \"Rare</p>\": 2})    \n",
    "\n",
    "#Mergeing Identical columns, final cleaning\n",
    "\n",
    "df=df.replace(to_replace =\"Non-Sporting\", \n",
    "                 value =\"Non-sporting\") \n",
    "df=df.replace(to_replace =\"companion\", \n",
    "                 value =\"Non-sporting\") \n",
    "df=df.replace(to_replace =\"Working.\", \n",
    "                 value =\"Working\") \n",
    "\n",
    "df=df.replace(to_replace =\"Toy.\", \n",
    "                 value =\"Toy\") \n",
    "\n",
    "\n",
    "df=df.replace(to_replace =\"Terrier.\", \n",
    "                 value =\"Terrier\") \n",
    "\n",
    "\n",
    "df=df.replace(to_replace =\"Hound.\", \n",
    "                 value =\"Hound\")    \n",
    "\n",
    "df= df.drop(['UKC_Class', \"Name\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping AKC breeds inorder to reduce classes, scores proved unable to predict difference between group 3 inputs, so merged them into 1.\n",
    "group1=Toy\n",
    "group2=Working, Sporting\n",
    "group3=Terrier, Herding, Hound, Non-sporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target']=['group1' if i ==\"Toy\" else \"group2\" if i == \"Working\" or i == \"Sporting\" else \"group3\" for i in df.AKC_Class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(\"AKC_Class\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting X and Y data for all models.\n",
    "Y= df.target\n",
    "X= df.drop(\"target\", axis=1)\n",
    "for i in list(X.columns):\n",
    "    X[i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting Data into test train, 20% test size.\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,Y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for evaluating\n",
    "\n",
    "def evaluate(test, pred, model):\n",
    "    return [model, precision_score(test, pred, average= None), recall_score(test, pred, average= None), accuracy_score(test, pred), f1_score(test, pred, average= None)]\n",
    "# print metrics\n",
    "def print_metrics(labels, preds):\n",
    "    print(\"Precision Score: {}\".format(precision_score(labels, preds, average= None)))\n",
    "    print(\"Recall Score: {}\".format(recall_score(labels, preds, average= None)))\n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(labels, preds)))\n",
    "    print(\"F1 Score: {}\".format(f1_score(labels, preds, average= None)))\n",
    "# plot confusion metrix\n",
    "def conf_metrix(model):\n",
    "    # Create the basic matrix\n",
    "    plt.imshow(model,  cmap=plt.cm.Blues)\n",
    "    # Add title and axis labels\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    # Add appropriate axis scales\n",
    "    class_names = set(y) # Get class labels to add to matrix\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    # Add labels to each cell\n",
    "    thresh = model.max() / 2. # Used for text coloring below\n",
    "    # Here we iterate through the confusion matrix and append labels to our visualization\n",
    "    for i, j in itertools.product(range(model.shape[0]), range(model.shape[1])):\n",
    "            plt.text(j, i, model[i, j],\n",
    "                     horizontalalignment='center',\n",
    "                     color='white' if model[i, j] > thresh else 'black')\n",
    "    # Add a legend\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf= DecisionTreeClassifier(max_depth= 10)\n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf_pred= clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearchCV(model):\n",
    "    param_grid={\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [None, 5, 10, 12],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 3, 4, 5, 6]}\n",
    "    return GridSearchCV(model,param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf_grid=gridsearchCV(tree_clf)\n",
    "tree_clf_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf_grid_pred= tree_clf_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_test, tree_clf_grid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DOT data\n",
    "dot_data = export_graphviz(clf, out_file=None,\n",
    "                           feature_names=X_train.columns,\n",
    "                           class_names=np.unique(Y).astype('str'),\n",
    "                           filled=True, rounded=True, special_characters=True)\n",
    "# Draw graph\n",
    "graph = graph_from_dot_data(dot_data)\n",
    "# Show graph\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.transform(X_train)\n",
    "scaled_data_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert into a DataFrame\n",
    "scaled_df_train = pd.DataFrame(scaled_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate KNeighborsClassifier\n",
    "Kclf = KNeighborsClassifier(n_neighbors= 7)\n",
    "\n",
    "# Fit the classifier\n",
    "Kclf.fit(scaled_data_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "test_preds = Kclf.predict(scaled_data_test)\n",
    "\n",
    "KNN_eva=evaluate(y_test, test_preds, 'KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find the best KNN formula w/ Micro averaging\n",
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25):\n",
    "    best_k = 0\n",
    "    best_score = 0.0\n",
    "    for k in range(min_k, max_k+1, 1):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        preds = knn.predict(X_test)\n",
    "        f1 = f1_score(y_test, preds, average=\"micro\")\n",
    "        if f1 > best_score:\n",
    "            best_k = k\n",
    "            best_score = f1\n",
    "    print(\"Best Value for k: {}\".format(best_k))\n",
    "    print(\"F1-Score: {}\".format(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_k(scaled_data_train, y_train, scaled_data_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Trees Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sds=clf.feature_importances_\n",
    "def plot_feature_importances(model):\n",
    "    n_features = X_train.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X_train.columns.values) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "\n",
    "plot_feature_importances(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Accuracy for Decision Tree Classifier: {:.4}%\".format(accuracy_score(y_test, pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating an BaggingClassifier\n",
    "bagged_tree =  BaggingClassifier(DecisionTreeClassifier(criterion='gini', max_depth=7), \n",
    "                                 n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagged_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagg_pred= bagged_tree.predict(X_test)\n",
    "bagg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Baggin_eva=evaluate(y_test, bagg_pred, 'Baggin')\n",
    "Baggin_eva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit a RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth= 7)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pred=forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Forrests_eva=evaluate(y_test, forest_pred, 'Random Forrests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost and Gradient Boost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an AdaBoostClassifier\n",
    "adaboost_clf = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Instantiate an GradientBoostingClassifier\n",
    "gbt_clf = GradientBoostingClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_clf.fit(X_train, y_train)\n",
    "gbt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost model predictions\n",
    "adaboost_train_preds = adaboost_clf.predict(X_train)\n",
    "adaboost_test_preds = adaboost_clf.predict(X_test)\n",
    "\n",
    "# GradientBoosting model predictions\n",
    "gbt_clf_train_preds = gbt_clf.predict(X_train)\n",
    "gbt_clf_test_preds = gbt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADA_eva=evaluate(y_test, adaboost_test_preds, 'AdaBoost')\n",
    "GBT_eva=evaluate(y_test, gbt_clf_test_preds, 'GBT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to test Adaboost and gbt boost test vs train scores\n",
    "def display_acc_and_f1_score(true, preds, model_name):\n",
    "    acc = accuracy_score(true, preds)\n",
    "    f1 = f1_score(true, preds, average='micro')\n",
    "    print(\"Model: {}\".format(model_name))\n",
    "    print(\"Accuracy: {}\".format(acc))\n",
    "    print(\"F1-Score: {}\".format(f1))\n",
    "    \n",
    "print(\"Training Metrics\")\n",
    "display_acc_and_f1_score(y_train, adaboost_train_preds, model_name='AdaBoost')\n",
    "print(\"\")\n",
    "display_acc_and_f1_score(y_train, gbt_clf_train_preds, model_name='Gradient Boosted Trees')\n",
    "print(\"\")\n",
    "print(\"Testing Metrics\")\n",
    "display_acc_and_f1_score(y_test, adaboost_test_preds, model_name='AdaBoost')\n",
    "print(\"\")\n",
    "display_acc_and_f1_score(y_test, gbt_clf_test_preds, model_name='Gradient Boosted Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_confusion_matrix = confusion_matrix(y_test, adaboost_test_preds)\n",
    "adaboost_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_confusion_matrix = confusion_matrix(y_test, gbt_clf_test_preds)\n",
    "gbt_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_classification_report = classification_report(y_test, adaboost_test_preds)\n",
    "print(adaboost_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_classification_report = classification_report(y_test, gbt_clf_test_preds)\n",
    "print(gbt_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean GBT Cross-Val Score (k=5):')\n",
    "print(cross_val_score(gbt_clf, X, Y, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate XGBClassifier\n",
    "XG_clf = XGBClassifier()\n",
    "\n",
    "# Fit XGBClassifier\n",
    "XG_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "training_preds = XG_clf.predict(X_train)\n",
    "XG_test_preds = XG_clf.predict(X_test)\n",
    "\n",
    "# Accuracy of training and test sets\n",
    "training_accuracy = accuracy_score(y_train, training_preds)\n",
    "test_accuracy = accuracy_score(y_test, test_preds)\n",
    "\n",
    "print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\n",
    "print('Validation accuracy: {:.4}%'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'max_depth': [7],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'n_estimators': [100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf = GridSearchCV(clf, param_grid, scoring='accuracy', cv=None, n_jobs=1)\n",
    "grid_clf.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = grid_clf.best_params_\n",
    "\n",
    "print('Grid Search found the following optimal parameters: ')\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print('%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "training_preds = grid_clf.predict(X_train)\n",
    "test_preds = grid_clf.predict(X_test)\n",
    "training_accuracy = accuracy_score(y_train, training_preds)\n",
    "test_accuracy = accuracy_score(y_test, test_preds)\n",
    "\n",
    "print('')\n",
    "print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\n",
    "print('Validation accuracy: {:.4}%'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_eva=evaluate(y_test, XG_test_preds, 'XGBoost')\n",
    "XGBoost_eva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(kernel='linear')\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pred= svm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, svm_pred)\n",
    "SVM_eva=evaluate(y_test, svm_pred, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler().fit(X_train)\n",
    "X_train_sca = min_max.transform(X_train)\n",
    "X_test_sca = min_max.transform(X_test)\n",
    "\n",
    "# created model\n",
    "# check the running time\n",
    "import time\n",
    "start = time.time()\n",
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(X_train_sca, y_train)\n",
    "log_runtime = time.time() - start\n",
    "\n",
    "# prediction of  the test set\n",
    "log_pred = log_clf.predict(X_test_sca)\n",
    "\n",
    "# evaluated metrix for result table\n",
    "log_eva = evaluate(y_test, log_pred, 'Logistic Regression ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a matrix of all test data\n",
    "log_eva = evaluate(y_test, log_pred, 'Logistic Regression ')\n",
    "Tree_eva=evaluate(y_test, clf_grid_pred, 'Decision Tree')\n",
    "Baggin_eva=evaluate(y_test, bagg_pred, 'Baggin')\n",
    "Forrests_eva=evaluate(y_test, forest_pred, 'Random Forrests')\n",
    "KNN_eva=evaluate(y_test, test_preds, 'KNN')\n",
    "ADA_eva=evaluate(y_test, adaboost_test_preds, 'AdaBoost')\n",
    "GBT_eva=evaluate(y_test, gbt_clf_test_preds, 'GBT')\n",
    "XGBoost_eva=evaluate(y_test, XG_test_preds, 'XGBoost')\n",
    "SVM_eva=evaluate(y_test, svm_pred, 'SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results= pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results= results.append([log_eva, Tree_eva, Baggin_eva, Forrests_eva, KNN_eva, ADA_eva, \n",
    "                         GBT_eva, XGBoost_eva, SVM_eva])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.columns=[\"Model\", \"Precision Score\", \"Recall Score\", \"Accuracy Score\", \"F1 Score\"]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting matrix with Weighted Scores\n",
    "def evaluate_weighted(test, pred, model):\n",
    "    return [model, precision_score(test, pred, average= \"weighted\"), recall_score(test, pred, average= \"weighted\"),\n",
    "            accuracy_score(test, pred), f1_score(test, pred, average= \"weighted\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_eva2 = evaluate_weighted(y_test, log_pred, 'Logistic Regression ')\n",
    "Tree_eva2=evaluate_weighted(y_test, clf_grid_pred, 'Decision Tree')\n",
    "Baggin_eva2=evaluate_weighted(y_test, bagg_pred, 'Baggin')\n",
    "Forrests_eva2=evaluate_weighted(y_test, forest_pred, 'Random Forrests')\n",
    "KNN_eva2=evaluate_weighted(y_tesresults_weighted= pd.DataFrame()t, test_preds, 'KNN')\n",
    "ADA_eva2=evaluate_weighted(y_test, adaboost_test_preds, 'AdaBoost')\n",
    "GBT_eva2=evaluate_weighted(y_test, gbt_clf_test_preds, 'GBT')\n",
    "XGBoost_eva2=evaluate_weighted(y_test, XG_test_preds, 'XGBoost')\n",
    "SVM_eva2=evaluate_weighted(y_test, svm_pred, 'SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_weighted= pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_weighted= results_weighted.append([log_eva2, Tree_eva2,\n",
    "                                           Baggin_eva2, Forrests_eva2, KNN_eva2, ADA_eva2, GBT_eva2, \n",
    "                                           XGBoost_eva2, SVM_eva2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_weighted.columns=[\"Model\", \"Precision Score\", \"Recall Score\", \"Accuracy Score\", \"F1 Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_weighted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
